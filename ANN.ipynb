{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.read_csv('Churn_Modelling.csv')\n",
    "X=dataset.iloc[:,3:-1].values\n",
    "Y=dataset.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "le=LabelEncoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:,2]=le.fit_transform(X[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One Hot Encoding of \"Geography Column\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct=ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[1])],remainder='passthrough')\n",
    "X=np.array(ct.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the dataset into the training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test,Y_train,Y_test=train_test_split(X,Y,train_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann=tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the input Layer and the first hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1106 16:45:28.442757 15268 deprecation.py:506] From c:\\users\\hp\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=6,activation='relu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the second hidden layer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=6,activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the ouptut layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.compile(optimizer='adam' ,loss='binary_crossentropy' ,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2000/2000 [==============================] - 0s 61us/sample - loss: 0.4432 - acc: 0.7995\n",
      "Epoch 2/100\n",
      "2000/2000 [==============================] - 0s 87us/sample - loss: 0.4391 - acc: 0.7995\n",
      "Epoch 3/100\n",
      "2000/2000 [==============================] - 0s 90us/sample - loss: 0.4355 - acc: 0.7990\n",
      "Epoch 4/100\n",
      "2000/2000 [==============================] - 0s 90us/sample - loss: 0.4319 - acc: 0.7990\n",
      "Epoch 5/100\n",
      "2000/2000 [==============================] - 0s 90us/sample - loss: 0.4284 - acc: 0.8020\n",
      "Epoch 6/100\n",
      "2000/2000 [==============================] - 0s 91us/sample - loss: 0.4255 - acc: 0.8050\n",
      "Epoch 7/100\n",
      "2000/2000 [==============================] - 0s 82us/sample - loss: 0.4228 - acc: 0.8100\n",
      "Epoch 8/100\n",
      "2000/2000 [==============================] - 0s 86us/sample - loss: 0.4204 - acc: 0.8180\n",
      "Epoch 9/100\n",
      "2000/2000 [==============================] - 0s 93us/sample - loss: 0.4182 - acc: 0.8180\n",
      "Epoch 10/100\n",
      "2000/2000 [==============================] - 0s 87us/sample - loss: 0.4160 - acc: 0.8190\n",
      "Epoch 11/100\n",
      "2000/2000 [==============================] - 0s 67us/sample - loss: 0.4142 - acc: 0.8190\n",
      "Epoch 12/100\n",
      "2000/2000 [==============================] - 0s 84us/sample - loss: 0.4130 - acc: 0.8225\n",
      "Epoch 13/100\n",
      "2000/2000 [==============================] - 0s 89us/sample - loss: 0.4111 - acc: 0.8225\n",
      "Epoch 14/100\n",
      "2000/2000 [==============================] - 0s 81us/sample - loss: 0.4096 - acc: 0.8235\n",
      "Epoch 15/100\n",
      "2000/2000 [==============================] - 0s 91us/sample - loss: 0.4082 - acc: 0.8250\n",
      "Epoch 16/100\n",
      "2000/2000 [==============================] - 0s 70us/sample - loss: 0.4066 - acc: 0.8285\n",
      "Epoch 17/100\n",
      "2000/2000 [==============================] - 0s 77us/sample - loss: 0.4048 - acc: 0.8280\n",
      "Epoch 18/100\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.4074 - acc: 0.824 - 0s 89us/sample - loss: 0.4034 - acc: 0.8280\n",
      "Epoch 19/100\n",
      "2000/2000 [==============================] - 0s 74us/sample - loss: 0.4020 - acc: 0.8320\n",
      "Epoch 20/100\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.4021 - acc: 0.830 - 0s 85us/sample - loss: 0.4004 - acc: 0.8315\n",
      "Epoch 21/100\n",
      "2000/2000 [==============================] - 0s 100us/sample - loss: 0.3993 - acc: 0.8295\n",
      "Epoch 22/100\n",
      "2000/2000 [==============================] - 0s 93us/sample - loss: 0.3978 - acc: 0.8310\n",
      "Epoch 23/100\n",
      "2000/2000 [==============================] - 0s 92us/sample - loss: 0.3972 - acc: 0.8330\n",
      "Epoch 24/100\n",
      "2000/2000 [==============================] - 0s 82us/sample - loss: 0.3952 - acc: 0.8325\n",
      "Epoch 25/100\n",
      "2000/2000 [==============================] - 0s 92us/sample - loss: 0.3940 - acc: 0.8325\n",
      "Epoch 26/100\n",
      "2000/2000 [==============================] - 0s 92us/sample - loss: 0.3924 - acc: 0.8320\n",
      "Epoch 27/100\n",
      "2000/2000 [==============================] - 0s 72us/sample - loss: 0.3910 - acc: 0.8320\n",
      "Epoch 28/100\n",
      "2000/2000 [==============================] - 0s 85us/sample - loss: 0.3901 - acc: 0.8315\n",
      "Epoch 29/100\n",
      "2000/2000 [==============================] - 0s 92us/sample - loss: 0.3888 - acc: 0.8345\n",
      "Epoch 30/100\n",
      "2000/2000 [==============================] - 0s 87us/sample - loss: 0.3872 - acc: 0.8345\n",
      "Epoch 31/100\n",
      "2000/2000 [==============================] - 0s 82us/sample - loss: 0.3857 - acc: 0.8355\n",
      "Epoch 32/100\n",
      "2000/2000 [==============================] - 0s 80us/sample - loss: 0.3843 - acc: 0.8370\n",
      "Epoch 33/100\n",
      "2000/2000 [==============================] - 0s 87us/sample - loss: 0.3830 - acc: 0.8360\n",
      "Epoch 34/100\n",
      "2000/2000 [==============================] - 0s 89us/sample - loss: 0.3812 - acc: 0.8395\n",
      "Epoch 35/100\n",
      "2000/2000 [==============================] - 0s 67us/sample - loss: 0.3797 - acc: 0.8415\n",
      "Epoch 36/100\n",
      "2000/2000 [==============================] - 0s 70us/sample - loss: 0.3780 - acc: 0.8425\n",
      "Epoch 37/100\n",
      "2000/2000 [==============================] - 0s 90us/sample - loss: 0.3763 - acc: 0.8405\n",
      "Epoch 38/100\n",
      "2000/2000 [==============================] - 0s 89us/sample - loss: 0.3751 - acc: 0.8445\n",
      "Epoch 39/100\n",
      "2000/2000 [==============================] - 0s 83us/sample - loss: 0.3733 - acc: 0.8470\n",
      "Epoch 40/100\n",
      "2000/2000 [==============================] - 0s 87us/sample - loss: 0.3715 - acc: 0.8490\n",
      "Epoch 41/100\n",
      "2000/2000 [==============================] - 0s 84us/sample - loss: 0.3703 - acc: 0.8480\n",
      "Epoch 42/100\n",
      "2000/2000 [==============================] - 0s 90us/sample - loss: 0.3683 - acc: 0.8530\n",
      "Epoch 43/100\n",
      "2000/2000 [==============================] - 0s 85us/sample - loss: 0.3664 - acc: 0.8555\n",
      "Epoch 44/100\n",
      "2000/2000 [==============================] - 0s 83us/sample - loss: 0.3654 - acc: 0.85650s - loss: 0.3667 - acc: 0.85\n",
      "Epoch 45/100\n",
      "2000/2000 [==============================] - 0s 69us/sample - loss: 0.3635 - acc: 0.8580\n",
      "Epoch 46/100\n",
      "2000/2000 [==============================] - 0s 78us/sample - loss: 0.3621 - acc: 0.8575\n",
      "Epoch 47/100\n",
      "2000/2000 [==============================] - 0s 86us/sample - loss: 0.3605 - acc: 0.8590\n",
      "Epoch 48/100\n",
      "2000/2000 [==============================] - 0s 97us/sample - loss: 0.3591 - acc: 0.8600\n",
      "Epoch 49/100\n",
      "2000/2000 [==============================] - 0s 84us/sample - loss: 0.3579 - acc: 0.8600\n",
      "Epoch 50/100\n",
      "2000/2000 [==============================] - 0s 74us/sample - loss: 0.3563 - acc: 0.8605\n",
      "Epoch 51/100\n",
      "2000/2000 [==============================] - 0s 85us/sample - loss: 0.3554 - acc: 0.8640\n",
      "Epoch 52/100\n",
      "2000/2000 [==============================] - 0s 87us/sample - loss: 0.3543 - acc: 0.8625\n",
      "Epoch 53/100\n",
      "2000/2000 [==============================] - 0s 65us/sample - loss: 0.3530 - acc: 0.8645\n",
      "Epoch 54/100\n",
      "2000/2000 [==============================] - 0s 70us/sample - loss: 0.3520 - acc: 0.8665\n",
      "Epoch 55/100\n",
      "2000/2000 [==============================] - 0s 78us/sample - loss: 0.3514 - acc: 0.8670\n",
      "Epoch 56/100\n",
      "2000/2000 [==============================] - 0s 71us/sample - loss: 0.3501 - acc: 0.8670\n",
      "Epoch 57/100\n",
      "2000/2000 [==============================] - 0s 71us/sample - loss: 0.3493 - acc: 0.8655\n",
      "Epoch 58/100\n",
      "2000/2000 [==============================] - 0s 72us/sample - loss: 0.3486 - acc: 0.8645\n",
      "Epoch 59/100\n",
      "2000/2000 [==============================] - 0s 71us/sample - loss: 0.3479 - acc: 0.8670\n",
      "Epoch 60/100\n",
      "2000/2000 [==============================] - ETA: 0s - loss: 0.3408 - acc: 0.870 - 0s 71us/sample - loss: 0.3472 - acc: 0.8650\n",
      "Epoch 61/100\n",
      "2000/2000 [==============================] - 0s 74us/sample - loss: 0.3460 - acc: 0.8690\n",
      "Epoch 62/100\n",
      "2000/2000 [==============================] - 0s 76us/sample - loss: 0.3459 - acc: 0.8670\n",
      "Epoch 63/100\n",
      "2000/2000 [==============================] - 0s 75us/sample - loss: 0.3458 - acc: 0.8690\n",
      "Epoch 64/100\n",
      "2000/2000 [==============================] - 0s 61us/sample - loss: 0.3452 - acc: 0.8680\n",
      "Epoch 65/100\n",
      "2000/2000 [==============================] - 0s 68us/sample - loss: 0.3446 - acc: 0.8680\n",
      "Epoch 66/100\n",
      "2000/2000 [==============================] - 0s 59us/sample - loss: 0.3436 - acc: 0.8690\n",
      "Epoch 67/100\n",
      "2000/2000 [==============================] - 0s 66us/sample - loss: 0.3435 - acc: 0.8710\n",
      "Epoch 68/100\n",
      "2000/2000 [==============================] - 0s 74us/sample - loss: 0.3432 - acc: 0.8695\n",
      "Epoch 69/100\n",
      "2000/2000 [==============================] - 0s 76us/sample - loss: 0.3431 - acc: 0.8695\n",
      "Epoch 70/100\n",
      "2000/2000 [==============================] - 0s 92us/sample - loss: 0.3428 - acc: 0.8700\n",
      "Epoch 71/100\n",
      "2000/2000 [==============================] - 0s 70us/sample - loss: 0.3427 - acc: 0.8710\n",
      "Epoch 72/100\n",
      "2000/2000 [==============================] - 0s 64us/sample - loss: 0.3424 - acc: 0.8700\n",
      "Epoch 73/100\n",
      "2000/2000 [==============================] - 0s 66us/sample - loss: 0.3418 - acc: 0.8705\n",
      "Epoch 74/100\n",
      "2000/2000 [==============================] - 0s 75us/sample - loss: 0.3412 - acc: 0.8710\n",
      "Epoch 75/100\n",
      "2000/2000 [==============================] - 0s 72us/sample - loss: 0.3414 - acc: 0.8710\n",
      "Epoch 76/100\n",
      "2000/2000 [==============================] - 0s 70us/sample - loss: 0.3409 - acc: 0.8705\n",
      "Epoch 77/100\n",
      "2000/2000 [==============================] - 0s 71us/sample - loss: 0.3412 - acc: 0.8720\n",
      "Epoch 78/100\n",
      "2000/2000 [==============================] - 0s 65us/sample - loss: 0.3407 - acc: 0.8715\n",
      "Epoch 79/100\n",
      "2000/2000 [==============================] - 0s 72us/sample - loss: 0.3404 - acc: 0.8710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100\n",
      "2000/2000 [==============================] - 0s 69us/sample - loss: 0.3402 - acc: 0.8715\n",
      "Epoch 81/100\n",
      "2000/2000 [==============================] - 0s 85us/sample - loss: 0.3407 - acc: 0.8715\n",
      "Epoch 82/100\n",
      "2000/2000 [==============================] - 0s 74us/sample - loss: 0.3398 - acc: 0.8690\n",
      "Epoch 83/100\n",
      "2000/2000 [==============================] - 0s 78us/sample - loss: 0.3397 - acc: 0.8720\n",
      "Epoch 84/100\n",
      "2000/2000 [==============================] - 0s 76us/sample - loss: 0.3393 - acc: 0.8720\n",
      "Epoch 85/100\n",
      "2000/2000 [==============================] - 0s 63us/sample - loss: 0.3391 - acc: 0.8720\n",
      "Epoch 86/100\n",
      "2000/2000 [==============================] - 0s 92us/sample - loss: 0.3385 - acc: 0.8725\n",
      "Epoch 87/100\n",
      "2000/2000 [==============================] - 0s 76us/sample - loss: 0.3389 - acc: 0.8710\n",
      "Epoch 88/100\n",
      "2000/2000 [==============================] - 0s 89us/sample - loss: 0.3384 - acc: 0.8720\n",
      "Epoch 89/100\n",
      "2000/2000 [==============================] - 0s 91us/sample - loss: 0.3392 - acc: 0.8710\n",
      "Epoch 90/100\n",
      "2000/2000 [==============================] - 0s 87us/sample - loss: 0.3387 - acc: 0.8720\n",
      "Epoch 91/100\n",
      "2000/2000 [==============================] - 0s 90us/sample - loss: 0.3386 - acc: 0.8710\n",
      "Epoch 92/100\n",
      "2000/2000 [==============================] - 0s 86us/sample - loss: 0.3384 - acc: 0.8710\n",
      "Epoch 93/100\n",
      "2000/2000 [==============================] - 0s 73us/sample - loss: 0.3384 - acc: 0.8720\n",
      "Epoch 94/100\n",
      "2000/2000 [==============================] - 0s 90us/sample - loss: 0.3378 - acc: 0.8705\n",
      "Epoch 95/100\n",
      "2000/2000 [==============================] - 0s 73us/sample - loss: 0.3374 - acc: 0.8710\n",
      "Epoch 96/100\n",
      "2000/2000 [==============================] - 0s 82us/sample - loss: 0.3368 - acc: 0.8705\n",
      "Epoch 97/100\n",
      "2000/2000 [==============================] - 0s 74us/sample - loss: 0.3368 - acc: 0.8720\n",
      "Epoch 98/100\n",
      "2000/2000 [==============================] - 0s 83us/sample - loss: 0.3369 - acc: 0.8710\n",
      "Epoch 99/100\n",
      "2000/2000 [==============================] - 0s 86us/sample - loss: 0.3366 - acc: 0.8730\n",
      "Epoch 100/100\n",
      "2000/2000 [==============================] - 0s 74us/sample - loss: 0.3366 - acc: 0.8705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x26696d60be0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.fit(X_train,Y_train,batch_size=32, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05887698]]\n"
     ]
    }
   ],
   "source": [
    "print(ann.predict(sc.transform([[1,0,0,600,1,40,3,60000,2,1,1,50000]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
